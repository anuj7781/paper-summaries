# GeminiFS : A Companion File System for GPUs

Link to the paper : https://www.usenix.org/system/files/fast25-qiu.pdf

Presentation : https://www.youtube.com/watch?v=zE-zVu4huKM&t=7s&ab_channel=USENIX

## 🚀 Introduction: When Your GPU Needs a File System

You know that feeling when you're training a massive model—something LLM-scale—and you're chewing through gigabytes of input, weights, and activations like it's nothing. But then you hit that wall.
Not compute. Not bandwidth.
**Memory.**

Sure, GPUs have ultra-fast HBM, but it’s small and expensive. You can’t just keep scaling memory linearly with model size. And adding more host DRAM or GPUs? Not always feasible or economical.

That’s why there's growing interest in storage-backed memory expansion—letting GPUs reach out to fast NVMe SSDs to fetch and store data. Sounds ideal, right? NVMe is cheap, fast, and getting faster.

But here's the catch: the way GPUs traditionally talk to storage is broken.

**🧱 The Legacy Bottleneck**

Historically, moving data from disk to GPU looked like this:

1. CPU reads data from storage into DRAM.
2. CPU copies it over PCIe into GPU memory.

That’s a bounce buffer setup—high latency, lots of copying, and painful to scale. So we got smarter.

Enter GPUfs, Dragon, and GDS (GPUDirect Storage)—frameworks that allowed GPU threads to initiate I/O requests and in some cases even get direct DMA from NVMe to GPU memory.

Nice improvement, but the CPU still played coordinator. It was on the critical path for metadata access, I/O setup, and request orchestration.

And when thousands of GPU threads start requesting I/O? The CPU becomes the bottleneck.

**⚡ The Raw Speed vs. Usability Dilemma**

Then came GPU-centric designs like BaM that let GPUs directly submit NVMe commands via in-GPU queues. Performance? Fantastic. The CPU is completely out of the data and control path.

But… there’s a problem.

These GPU-centric models treat storage as a raw block device. That means:

- No file names, no directories
- No access control, no metadata
- No crash consistency or sharing

You get speed, but lose the file system abstraction we all rely on.

So developers are forced to choose:
Do you want speed (but no usability)?
Or usability (and take the performance hit)?

**🎯 Enter GeminiFS: The Best of Both Worlds**

This is the gap GeminiFS tries to close.

GeminiFS isn’t a replacement for your existing host file system. It’s a companion. A sidekick.

- The host FS still handles file creation, directory structure, metadata updates, permissions, crash safety.
- The GPU, with GeminiFS, gets a direct I/O path to NVMe for reading/writing file data—without bouncing through CPU DRAM or orchestration.

By splitting responsibilities this way, GeminiFS aims to get:

- The performance of BaM (GPU-centric, no CPU in the path)
- The convenience of POSIX (file handles, offsets, and caching)

But designing something like this is far from trivial.

**🧗 The Challenges**

The authors identify four core technical challenges GeminiFS had to overcome:

1. 🗂 Metadata Synchronization

How do you let the GPU get essential metadata (e.g., block maps) without needing to run a full file system?
Especially since syncing with the host FS on every read would destroy performance.

2. 🧩 Shared NVMe Driver

Standard NVMe drivers assume the host OS has exclusive control.
But GeminiFS needs both the CPU and GPU to share the same NVMe device—each with its own queue pairs.

3. 🔄 Page Cache on the GPU

GPUs have amazing memory bandwidth—why not cache pages locally?
But GPU memory is usually per-process, lacks privileged mode, and has high thread counts—creating cache contention and isolation problems.

4. 🧑‍💻 Usability for Developers

The complexity of coordinating host and GPU, managing metadata, issuing NVMe commands… could be overwhelming.
GeminiFS needs to abstract this away behind a simple API that feels as natural as read() and write().

**🧠 Helpful Workload Assumptions**

Luckily, ML workloads—especially for training—have predictable I/O patterns:

- Mostly large sequential reads (weights, inputs)

- Append-only writes (checkpoints, logs)

- Metadata is often static or known in advance

This predictability lets GeminiFS embed necessary metadata in files up front and avoid full-blown FS logic on the GPU.

## 🧩 Pattern 1: Embedding Device-Specific Metadata for Direct GPU Access

One of the smartest design choices in GeminiFS is not trying to make the GPU behave like a full-blown file system.
Why? Because GPUs are terrible at pointer-chasing, and most traditional file systems (like EXT4 or XFS) store metadata in structures like B-trees or extent trees that require traversals, conditionals, and branching — things GPUs don’t do well.

**🧠 So what’s the alternative?**
GeminiFS uses a strategy we’ll call:

**"Embed the essential metadata with the data."**

Rather than having the GPU walk the host FS structures, GeminiFS embeds a simplified, GPU-friendly metadata block inside the file itself.

This is where their custom file format comes in:

**🔖 GVDK – Gemini Virtual Disk**

Think of this like VMDK or QCOW2 virtual disk formats. When a file is created or registered for use with GeminiFS, it’s laid out in a GVDK structure:
- A header block containing:
  - File type
  - File size
  - I/O block size
  - Access mode
  - Block mapping tables
  - A dirty bitmap (for tracking updates)
- Then, the actual data blocks

**🔁 Two-Level Block Mapping: Fast Offset Translation**

At the heart of this format is a two-level mapping table:
- L1 Table → Points to blocks that hold...
- L2 Tables → Which store the physical NVMe block addresses (LBAs)

So when a GPU thread says: “Give me byte offset 1,048,576,”
GeminiFS quickly maps that to a block index → L2 entry → NVMe block offset.

No B-trees. No syscalls. Just a few direct lookups — and entirely GPU-friendly.

**🧠 Why does this matter?**

Because address translation is on the hot path of every read and write.
Making it simple and parallelizable is key to performance.

**🛠️ Metadata Helper on the Host**

Of course, the GPU doesn’t magically know where blocks live on disk.
There’s a GVDK helper module on the host side that, at file setup time:
- Queries the host file system for block mappings
- Embeds them into the file's GVDK header

This makes GPU reads completely independent of the CPU once initialized.

**📉 Overhead?**

They report about 0.2% space overhead from the embedded map. For example, each 4KB block mapping requires 8 bytes — negligible considering the performance benefits.

**💡 Takeaway for System Developers**

Key insight: If your accelerator needs to read structured data quickly, don’t force it to navigate general-purpose FS metadata.
Instead, embed just the minimal, critical metadata it needs for fast access, and structure it to match the accelerator’s strengths (e.g., table-based access for GPUs).
This is a concrete and elegant strategy for bridging the gap between standard host-managed file systems and high-performance, accelerator-driven I/O.

## 🛠 Pattern 2: Shared NVMe Control Planes — Letting GPU and CPU Co-Manage the Device

One of the boldest system-level changes in GeminiFS is deep in the kernel:
They modified the Linux NVMe driver to support parallel, shared control between the host CPU and the GPU.

**🧩 The Problem**

In standard Linux systems, the NVMe kernel driver assumes exclusive ownership of the NVMe device. It:
- Creates and manages admin queues
- Sets up and owns I/O submission (SQs) and completion queues (CQs)
- Registers the device with the block layer
- Talks to file systems like EXT4, XFS

All that works well for CPU-driven I/O, but completely excludes non-CPU devices like GPUs from directly participating.

**⚙️ The Insight**

The GPU doesn’t need full device control.
It only needs to:
- Submit NVMe I/O requests
- Poll for completions
- And do it entirely from GPU memory

  With this in mind, the authors built SNVMe — a Shared NVMe Driver that allows:
- The host OS to retain full administrative control over the device (namespaces, mounts, etc.)
- The GPU to establish its own I/O queues in GPU memory and issue read/write commands directly

**🔄 How It Works: From GPU to NVMe**

Here’s the flow:

1. GPU Allocates I/O Queues in Its Own Memory
The GPU creates submission and completion queues (SQ/CQ) in HBM.

2. Driver Support to Pin GPU Memory for DMA
SNVMe uses NVIDIA APIs like:
- nvidia_p2p_get_pages_persistent()
- nvidia_p2p_dma_map_pages()

These allow the host to:
- Pin the GPU memory
- Translate GPU virtual addresses into DMA-capable physical addresses

This makes GPU memory visible and accessible to the NVMe controller.

3. GPU Issues NVMe Commands Directly

GPU threads write commands into the SQs
The NVMe device fetches them over PCIe and begins execution

4. GPU Polls Completion Queues

Instead of triggering CPU interrupts, the GPU polls its CQ memory region for completed commands

Thanks to the massive thread counts in modern GPUs, polling is efficient and scalable

**🤖 True CPU Bypass — Without Losing Control**

What’s impressive here is the balance:
- The host kernel still controls the device from an administrative standpoint
- The GPU has direct data path control for I/O performance

This design gives us:
- Parallel control planes: CPU and GPU each manage their own I/O queues
- Fine-grained coordination without shared locking or exclusive control
- Safety and coexistence with existing file systems

**💡 Takeaway for System Developers**
Key Insight: If your accelerator (GPU, FPGA, etc.) needs high-performance I/O, consider giving it dedicated I/O queues — not just buffers — and extend the kernel driver to manage multiple independent control paths.

This pattern of shared driver control is powerful:
- Enables true device-level heterogeneity
- Keeps host-kernel ownership intact
- Unlocks direct storage access for accelerators without giving up administrative safety

Whether you're building for GPUs, DPUs, or even NICs with local compute, this model shows a scalable path to cooperative device sharing at the driver level.


## ⚡ Pattern 3: Building a GPU-Specific Page Cache — Performance-Aware Design for Parallelism

Once you've enabled the GPU to directly issue NVMe reads and writes, the next logical performance frontier is caching.
But caching on the GPU? That's a whole different beast.

**🧠 Why Build a Page Cache on the GPU?**

Because GPU memory bandwidth is insane.

- HBM can hit hundreds of GB/s, sometimes nearing 1 TB/s
- PCIe Gen4 or even Gen5 to NVMe tops out around 14 GB/s

So if your data is already in the GPU, it's dramatically cheaper and faster to serve it from on-device memory than to re-read it over PCIe from disk. That’s a 6×–70× gap.

If you don’t cache at the GPU level, you’re wasting bandwidth and performance.

**🔥 Challenge 1: User-Space Scope and Sharing**

Unlike CPU page caches (kernel-global), a GPU’s page cache:
- Must be implemented in user space, inside the GPU process
- Can’t rely on privileged kernel modes
- Does not persist across GPU processes by default

**🧩 GeminiFS Solution: Cross-Process GPU Cache Sharing**

To avoid duplication when multiple processes access the same file:

- GeminiFS’s SNVMe module on the host side manages a shared metadata registry.
- When a process opens a file:
  - It checks if a page cache for that file already exists
  - If yes, it uses GPU IPC APIs (e.g., cuIpcGetMemHandle) to:
    - Share the same GPU memory pages
    - Let multiple processes map and reuse the same cache

This avoids redundant memory usage and allows coherent caching across processes — a big win for model sharing or multi-GPU parallelism.

**🔥 Challenge 2: High Contention from Thousands of Threads**

With hundreds of thousands of threads potentially accessing or modifying the cache simultaneously, lock contention could kill performance.

**🧩 GeminiFS Solution: Warp-Level Locking + Constant-Time Data Structures**

**✅ Warp-Level Locking**

- A warp (typically 32 GPU threads) executes in lockstep.
- GeminiFS acquires locks per warp, not per thread.
- This dramatically reduces the number of concurrent lock contenders — from 1,000s of threads to a few 100s of warps.

Fewer lock acquisitions → less contention → better scalability

**✅ Constant-Time Cache Operations**

GeminiFS designs its cache data structures for:

- Constant-time lookup (via hash tables)
- Fast eviction (via doubly linked lists for LRU logic)
- Efficient insertion/deletion (avoiding long critical sections)

These minimize how long each lock is held — which matters more than ever when thousands of threads are queued up.

**💡 System Design Takeaway**

Key Insight: High-parallelism environments (like GPUs) need you to rethink standard shared resource management.
Don’t just port CPU-style caches — redesign them for your hardware’s thread and memory model.

Concrete takeaways:
- Use coarse-grained synchronization, like warp-level locks
- Build constant-time critical operations to reduce time in locks
- Offload cross-process coordination to helper modules (e.g., via driver or runtime)

Whether you're building a cache, allocator, or buffer pool — these principles are broadly useful across accelerator system software.
