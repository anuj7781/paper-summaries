# BaM: GPU-Initiated High-Throughput Storage Access

Paper link: https://arxiv.org/pdf/2203.04910

## â— The Problem: CPUs Are Choking GPU Performance
Modern GPUs are computational beasts, capable of massive thread-level parallelism. But there's a catch: they need to be fedâ€”and the datasets in todayâ€™s workloads are too large to fit in GPU memory. This creates a critical bottleneck: how do we get massive datasets from storage to the GPU, efficiently and at scale?
This creates a critical bottleneck: how do we get massive datasets from storage to the GPU, efficiently and at scale?
**Traditional Answer: Let the CPU Orchestrate It**
Historically, GPUs rely on CPUs to:
- Preload data chunks (proactive tiling),
- Or respond to page faults via unified memory (reactive fetches).

Both approaches fall apart under data-dependent access patterns typical of:
- Graph analytics
- Recommender systems
- GNNs
- Data analytics pipelines

Why? Because:
- The CPU can't predict what the GPU will need.
- Tiling leads to I/O amplificationâ€”fetching way more data than necessary.
- Page faults introduce latency and overhead, with CPUs maxing out at ~500K IOPS, far below the millions that NVMe SSDs can handle.

**Analogy:**
Itâ€™s like having a racecar (the GPU) but refueling it with a teaspoon (the CPU).

**Net Result:**
- Underutilized storage hardware (PCIe Gen4 links, SSDs).
- CPU-GPU synchronization overhead.
- Stalled GPU kernels.
- Wasted bandwidth and energy.

**This is the status quo. The paper â€œGPU-Initiated On-Demand High-Throughput Storage Access in the BaM System Architectureâ€ challenges that by asking:**
What if the GPU could fetch its own dataâ€”directly, on demand, at full speed?.

ğŸ’¡ The BaM Solution: Bringing Storage Closer to the GPU
Thatâ€™s precisely the bottleneck BaM â€” short for Big Accelerator Memory â€” is designed to fix.

At its core, BaM isnâ€™t just a faster API or a minor optimization. Itâ€™s a new tier in the GPUâ€™s memory hierarchy, one that directly leverages NVMe storage as a semi-integrated extension of GPU memory.

## ğŸ’¡ The BaM Solution: Bringing Storage Closer to the GPU
Thatâ€™s precisely the bottleneck BaM â€” short for Big Accelerator Memory â€” is designed to fix.
At its core, BaM isnâ€™t just a faster API or a minor optimization. Itâ€™s a new tier in the GPUâ€™s memory hierarchy, one that directly leverages NVMe storage as a semi-integrated extension of GPU memory.

**ğŸš« No More CPU Middleman**

Traditionally, storage is something the CPU manages: queuing requests, managing caching, launching GPU kernels, and orchestrating data transfers. BaM flips this on its head.
With BaM: The GPU takes over the storage control path.
Itâ€™s the GPU that:
- Issues fine-grained I/O requests
- Manages queues
- Maintains the software cache
- Triggers doorbells to notify the SSD

This is a major architectural shift. GPU as not just a compute engine, but a self-sufficient data engine.

**ğŸ§± What Makes It Work: The Building Blocks of BaM**

![My diagram](assets/bam/fig-1.png)

To make this vision real, BaM introduces three key elements living in GPU memory:

**1. ğŸ§  A Fine-Grained Software Cache**

Implemented entirely in GPU DRAM, this cache:
- Minimizes redundant I/O
- Coalesces requests across threads
- Enables warp-level data reuse
Itâ€™s warp-aware and reference-counted â€” a smart, high-concurrency caching layer for out-of-core data.

**2. ğŸš¦ GPU-Resident Submission and Completion Queues**

These queues are:
- Allocated in GPU memory
- Filled directly by GPU threads
- Monitored by the SSD controller (via doorbell)
This enables GPU threads to issue storage I/O directly, in a massively parallel, lock-minimized fashion. They call this: GPU Kernel-Initiated Storage (GPUDirect Async KI)

**3. ğŸ§¾ The bam::array<T> Abstraction**

A C++-like API that:
- Feels like accessing regular arrays
- Transparently performs cache lookup
- Issues I/O on misses
- Returns data inline to the kernel
This lets developers write out-of-core GPU programs without major kernel rewrites or tiling logic.

**ğŸ§µ Letâ€™s Walk Through a Threadâ€™s Journey**

![My diagram](assets/bam/fig-2.png)

Imagine a GPU thread trying to read some element val = data[tid]; from a massive dataset:
- The access is routed through a bam::array abstraction.
- The system computes the cache line offset.
- Threads in the warp coalesce accesses (classic warp sync).
- If the data is cached, the thread proceeds. If not:
  - The thread builds an I/O command.
  - Submits it to an SQ (Submission Queue) in GPU memory.
  - Rings the SSDâ€™s doorbell register, which is mapped into GPU address space.
  - Polls a CQ (Completion Queue) for the result.

The SSD controller:
-  Fetches the command,
-  Performs a DMA read directly into GPU memory,
-  Posts a completion entry back.

The thread:
- Sees the CQ entry,
- Updates the cache metadata,
- Proceeds with computation.
No CPU intervention. No memory bounces. Direct storage â†’ GPU â†’ compute.

**ğŸ” Compare with the Old Way**

![My diagram](assets/bam/fig-3.png)

In traditional CPU-centric designs:
- The CPU reads data,
- Moves it to GPU,
- Launches a kernel,
- Waits, re-fetches data, and repeats.

With BaM:
- The GPU kernel runs once.
- It fetches what it needs, when it needs it.
- Storage latency can be overlapped with computation.

**ğŸ¯ The Payoff?**
- Minimal CPU-GPU synchronization.
- Drastic reduction in I/O amplification.
- Full use of PCIe/NVMe bandwidth â€” even at 4KB granularity.
- Performance close to expensive DRAM setups, at a fraction of the cost.

## ğŸ§  Under the Hood: Engineering BaM for Extreme GPU-Scale Concurrency
So hereâ€™s a natural question:
If thousands of GPU threads are all trying to submit I/O requests at the same time, how does BaM avoid chaos?

After all, traditional storage systems rely on careful locking and serialization â€” you donâ€™t want threads stepping on each other while updating queue pointers or ringing doorbells to notify the SSD.

**ğŸ”“ The Locking Problem at GPU Scale**
On a CPU, itâ€™s common to guard submission queues with locks. But that doesnâ€™t scale when youâ€™ve got tens of thousands of GPU threads working in parallel. A single lock would instantly become a bottleneck.

**ğŸ§© BaMâ€™s Solution: GPU-Friendly Concurrency Design**

Instead of traditional locks, BaM uses:
- Fine-grained memory synchronization
- Atomic operations
- Custom data structures designed for parallelism

**âœ… Submission Queue Logic (Simplified)**

Hereâ€™s how a thread submits an I/O request:

**1. Take a Ticket ğŸŸ**
A thread atomically increments a counter to get a unique "virtual slot number" â€” like pulling a number at a deli.

**2. Map to Queue Entry**
This ticket tells the thread which physical queue entry to use, and when itâ€™s their "turn" to write.

**3. Wait Smartly**
Threads donâ€™t wait in one giant line. They wait at their designated spot using a compact per-entry counter. This allows parallel access across many entries.

**4. Mark it Ready**
Once the thread writes its I/O command into the queue entry, it sets a bit in a shared bit vector to indicate it's ready.

**5. Batch and Notify ğŸ””**
Threads then race (briefly) for a small lock just to move the queue tail and ring the SSDâ€™s doorbell.
The winner batches multiple ready entries and sends a single doorbell write â€” optimizing PCIe usage.

**ğŸ”„ Completion Queues Work Similarly**

The same principles apply when handling completions. BaM minimizes contention and avoids blocking by using mark bits, atomic counters, and coalesced updates.

**ğŸ’¾ The BaM Software Cache: Smarter, Leaner, GPU-Native**
Even with efficient queues, hitting storage constantly is slow. So BaM introduces a GPU-resident software cache:

**Why Itâ€™s Special:**
- Fully pre-allocated at launch â€” no dynamic memory allocation headaches during execution.
- Fine-grained locking â€” locks apply per-cache-line, not globally.
- Reference-counted â€” tracks how many threads are using a line to prevent premature eviction.

**ğŸ‘¯ Handling Multiple Threads Missing on Same Data**
- The first thread that hits a miss locks that cache line and fetches data from storage.
- Other threads wait on the same cache line, avoiding redundant I/O.
- Once data is fetched and marked valid, everyone uses it and updates a reference count.

**â™»ï¸ Parallel Cache Eviction**
- BaM uses a variation of the clock replacement algorithm.
- Threads get a candidate slot by incrementing a global counter.
- If itâ€™s unpinned (refcount = 0), they can evict and reuse it.
- If pinned, they try the next â€” enabling parallel eviction across threads.

**ğŸ¯ Warp-Level Optimizations: Coalescing at the Hardware Frontier**

GPUs execute in warps â€” groups of 32 threads running in lockstep. If multiple threads in a warp request the same cache line, BaM avoids waste by:
- Using intrinsics like __match_any_sync to detect which threads are requesting the same cache line.
- Electing a leader thread per group to handle the cache probe and possible miss resolution.
- Broadcasting results using __shfl_sync, so all group members can use the result.

This drastically reduces duplicate cache lookups and enhances performance.

**ğŸ’» All This, Behind an Array Interface**
Despite all this complexity, the programmer sees none of it. BaM exposes a simple abstraction:
```
bam::array<float> data;
val = data[tid];  // behind the scenes: cache lookup, I/O, coalescing
```
Everything â€” cache checks, I/O submission, warp-level optimizations â€” is hidden behind an overloaded operator[].

## ğŸ“Š Real-World Evaluation: BaM Under Pressure
The BaM team didnâ€™t just propose a bold architecture â€” they built and tested a fully working prototype using commodity hardware and a custom I/O software stack running entirely on the GPU.

**ğŸ§ª Hardware Setup**
- GPU: NVIDIA A100 (PCIe Gen4 x16)
- Storage: Mix of high-end and consumer SSDs
  - Intel Optane P5800X (low-latency, high-endurance)
  - Samsung PM1735 (Z-NAND)
  - Samsung 980 Pro (consumer NVMe)
- System Design: Multi-SSD PCIe expansion chassis and riser cards to saturate GPU PCIe bandwidth
- Software Stack:
  - Custom Linux char driver
  - GPUDirect RDMA + Async: Enables the GPU to access NVMe submission/completion queues and ring doorbells directly
ğŸ’¡ Why this matters: All control logic â€” from command setup to doorbell notification â€” is done on the GPU. No kernel crossings, no CPU intervention.

**ğŸ’° Cost Analysis: BaM vs DRAM Systems**
| Technology          | Read IOPS (512B) | Write IOPS (512B) | Latency  | $/GB   | Relative Cost   |
|---------------------|------------------|-------------------|----------|--------|-----------------|
| DRAM                | >10M             | >10M              | ~0.1 Âµs  | $11.13 | 1Ã—              |
| Optane              | 5.1M             | 1M                | ~10 Âµs   | $2.54  | 4.4Ã— cheaper    |
| Z-NAND              | 1.1M             | 351K              | ~25 Âµs   | $2.56  | 4.3Ã— cheaper    |
| Samsung 980 Pro     | 700â€“800K         | 172K              | ~100 Âµs  | $0.51  | 21.8Ã— cheaper   |

ğŸ“ Why BaM wins here: SSDs offer orders-of-magnitude better cost per GB than DRAM, and BaM makes them usable at high performance even for fine-grained, GPU-bound workloads.

**ğŸš€ Microbenchmark Performance**
BaMâ€™s raw I/O throughput, using up to 10 Intel Optane SSDs:
| Config           | Random Read IOPS | Random Write IOPS | Bandwidth                               |
|------------------|------------------|--------------------|------------------------------------------|
| 10 Optane SSDs   | 45.8M IOPS       | 10.6M IOPS         | 22.9 GB/s (90% of PCIe Gen4 x16 limit)   |

ğŸ’¡ Why it works: BaM uses fine-grained atomic sync, warp-aware queuing, and coalesced doorbell updates â€” avoiding PCIe overhead and enabling massive concurrency across thousands of GPU threads.

**âš”ï¸ Comparison With Existing Solutions**
1. NVIDIA GDS (GPU Direct Storage)

| Metric                   | ActivePointers (CPU-managed DRAM) | BaM (SSD, GPU-managed)       |
|--------------------------|-----------------------------------|-------------------------------|
| Cache Miss Handling Rate | 823K IOPS                         | 17M IOPS (20.7Ã— faster)       |
| Hot Cache Bandwidth      | 38.4 GB/s                         | 430 GB/s (11.2Ã— faster)       |

ğŸ” Why BaM outperforms: GDS relies on the CPU to batch I/Os and initiate DMA. This creates inefficiency for small, random access. BaMâ€™s GPU-based queues remove this bottleneck entirely.

**2. ActivePointers + GPUfs (Prior Research)**

| Metric                   | ActivePointers (CPU-managed DRAM) | BaM (SSD, GPU-managed)       |
|--------------------------|-----------------------------------|-------------------------------|
| Cache Miss Handling Rate | 823K IOPS                         | 17M IOPS (20.7Ã— faster)       |
| Hot Cache Bandwidth      | 38.4 GB/s                         | 430 GB/s (11.2Ã— faster)       |

ğŸ” Why BaM outperforms:
- AP still funnels cache miss handling through the CPU (via GPUfs).
- BaM avoids this by letting GPU threads directly issue and complete I/O.
- Even with slower SSDs, BaMâ€™s parallelism and direct DMA win over CPU-based DRAM reads.

**ğŸ” Key Takeaways for Storage Devs**
No CPU-side bottlenecks: All I/O queuing, polling, and cache management is done on the GPU.
- Highly concurrent queues: Designed from scratch using atomics and warp-aware coordination â€” not legacy kernel spinlocks.
- Low-latency + High-throughput: BaM leverages doorbell coalescing and queue slot partitioning to maximize PCIe efficiency.
- Tiny I/O ready: Unlike GDS, BaM handles 4KB random accesses at line rate â€” essential for graph workloads and vector DBs.
- Cost-effective scaling: Consumer SSDs can be used at near-optimal performance with the right concurrency infrastructure.

## Evaluation Part 2: Application Workloads â€“ Graph Analytics
After validating BaMâ€™s microbenchmark performance, the authors turned to real-world applications â€” specifically graph analytics:
- Breadth-First Search (BFS)
- Connected Components (CC)
These are classic data-intensive workloads with unpredictable, sparse access patterns â€” ideal stress tests for storage systems.

**ğŸ“š Setup**
Workloads: BFS and CC on five large graphs:
- GAP-kron (31.5 GB)
- GAP-urand (32.0 GB)
- Friendster (26.9 GB)
- MOLIERE_2016 (49.7 GB)
- uk-2007-05 (27.8 GB)

Baseline: DRAM-only system with all input graphs loaded into host memory
BaM: Four Intel Optane SSDs with 128 NVMe queues (depth 1024), and 8 GB GPU-side cache

**âš”ï¸ DRAM vs BaM: Surprising Results**
DRAM systems are expensive (21.7Ã— cost per GB) and supposedly faster â€” but BaM matches or beats them.

| Workload | BaM vs DRAM    | Explanation                    |
|----------|----------------|--------------------------------|
| BFS      | 1.00Ã— speedup  | Same end-to-end runtime        |
| CC       | 1.49Ã— speedup  | BaM was significantly faster   |


**ğŸ” Why BaM wins:**
- DRAM system pays a huge upfront cost to preload the full graph into memory.
- BaM starts computing immediately, fetching only whatâ€™s needed, overlapping I/O with compute.
- BaMâ€™s on-demand fetch model avoids I/O amplification and preloading overhead.

**ğŸ”¬ Breakdown of BaM Runtime**
BaMâ€™s execution is broken into three components:
- Compute time (same as DRAM)
- Cache access + metadata overhead
- Storage I/O time

**Key findings:**
- Cache overhead ranges from 2%â€“45% depending on the SSD configuration.
- With 4 SSDs, BaM approaches 80â€“90% of peak IOPS and overlaps I/O with compute effectively.
- DRAM wins in raw bandwidth, but loses in data transfer efficiency and load time.

**âš™ï¸ Cache Optimizations: Big Wins for BaM**
BaM's cache design isn't just a buffer â€” it's optimized for the GPU architecture. Two levels of optimization are evaluated:

| Optimization                  | BFS Speedup | CC Speedup |
|------------------------------|-------------|------------|
| Naive Cache vs No Cache      | 12Ã—         | 12.65Ã—     |
| Optimized Cache vs Naive     | 6.07Ã—       | 11.24Ã—     |

ğŸ” Why it works:
Even a simple cache cuts down raw I/O drastically.
BaM adds GPU-specific tricks:
- Warp coalescing: Avoids redundant cache probes
- Reference counting: Enables reuse without refetch
- Parallel eviction (clock algorithm): Reduces contention
These are tuned for warp-level concurrency and fine-grained GPU parallelism, unlike traditional CPU caches.

**ğŸ§  Summary for Storage Developers**
- BaM doesnâ€™t â€œbrute forceâ€ I/O â€” it outsmarts it. DRAM offers higher bandwidth, but BaM wins by being selective and parallel.
- On-demand access avoids bulk prefetching and I/O amplification.
- Cache design matters: Fine-grained locking, warp-level coordination, and reference tracking are crucial for performance.
- Storage stack architecture matters: Queues, locks, and doorbells â€” when designed for the GPU â€” flip the expected performance hierarchy.

## Storage Matters: SSD Type and System Tuning
BaM lets the GPU talk directly to SSDs â€” but does the type of SSD significantly impact performance?

** ğŸ“‰ SSD Sensitivity **
The team tested BaM with three SSD classes:

| SSD Model              | Type                   | BFS Slowdown vs Optane | CC Slowdown vs Optane | Notes                        |
|------------------------|------------------------|-------------------------|------------------------|------------------------------|
| Intel Optane P5800X    | Enterprise, low-latency| Baseline                | Baseline               | High IOPS, low latency       |
| Samsung PM1735 (Z-NAND)| Enterprise, fast       | Similar performance     | Similar performance    | Comparable to Optane         |
| Samsung 980 Pro        | Consumer-grade NVMe    | 3Ã— slower (BFS)         | 2.7Ã— slower (CC)       | Lowest cost                  |


ğŸ” Why performance drops:
Consumer SSDs like the 980 Pro have lower random read throughput and higher latency. Since BaM is designed for fine-grained, demand-driven I/O, SSD latency and queue depth scalability directly affect total runtime.

ğŸ’¡ Trade-off Insight: 
BaM exposes a cost-performance knob â€” developers can choose between cheaper consumer SSDs or higher-end enterprise drives based on workload criticality and TCO targets.

** ğŸ”§ BaM Tunability: Cache and Queue Pairs ** 
BaM isn't fragile â€” it performs well across different configurations.

**ğŸ“¦ Cache Size Sensitivity**
Using the kron dataset:

| GPU Cache Size | Performance Impact    |
|----------------|------------------------|
| 8 GB â†’ 1 GB     | Minimal degradation    |


ğŸ” Why it holds up:
BaM's cache captures the essential working set even when reduced to 1 GB. This points to:
- Good locality in graph workloads
- Effective eviction policy (clock-based + reference counting)
- Warp-level coalescing reducing redundant fetches

**ğŸ”„ Queue Pair Sensitivity**
BaM tested varying numbers of Submission/Completion Queue (SQ/CQ) pairs:

| Num. of Q Pairs | Performance                   |
|------------------|-------------------------------|
| â‰¥ 40 pairs       | Stable, near-peak throughput  |
| < 40 pairs       | Noticeable drop-off           |

ğŸ” Why this matters:
GPU threads generate thousands of concurrent I/O ops. Having enough parallel SQ/CQ pairs ensures:
- Less contention
- More concurrent inflight I/Os
- Full SSD parallelism utilization

ğŸ’¡ The system stays robust until around 40 queue pairs, giving developers flexibility when tuning for hardware constraints.

**ğŸ“Œ Storage Developer Takeaways:**
- SSD quality directly affects fine-grained I/O throughput. For latency-sensitive workloads, Optane or Z-NAND class SSDs are ideal.
- BaM enables cost-performance tuning â€” run the same system with different SSD tiers.
- Cache size and Q pair counts are tunable knobs:
- Small caches still perform well if locality exists.
- 40 queue pairs recommended for full throughput.

## ğŸ“ˆ Data Analytics: Beating RAPIDS with SSDs
BaM wasn't just tested on graph workloads â€” the authors also evaluated it using data analytics queries on the large NYC Taxi dataset, a classic benchmark for real-world big data.

**âš–ï¸ The Comparison Setup**
- BaM: Ran queries directly from SSDs using fine-grained on-demand fetches.
- RAPIDS (baseline): Industry-standard GPU analytics framework, operating on data already pinned in CPU DRAM â€” bypassing storage latency entirely.
âš ï¸ Advantage: RAPIDS â€” but BaM still won.

ğŸš€ Performance Results

| Query               | BaM Speedup vs RAPIDS                      |
|---------------------|--------------------------------------------|
| Q5 (complex filter) | 5.3Ã— faster                                |
| Other queries (Q1â€“Q4) | Varying speedups, still favorable to BaM |


ğŸ” Why BaM wins:
- RAPIDS processes entire columns, even if most data is discarded.
- BaM fetches only the rows that match filters, avoiding I/O amplification.
- Lower CPU overhead and better GPU compute/I/O overlap also helped BaM even in simpler queries.

ğŸ“‰ I/O Amplification (Figure 14)

| System | Max I/O Amplification   |
|--------|--------------------------|
| RAPIDS | >6Ã—                      |
| BaM    | Near 1Ã— (ideal)          |


ğŸ’¡ Key Insight:
RAPIDSâ€™ columnar scan model wastes bandwidth when queries are highly selective. BaM's selective access model proves far more efficient as query complexity increases.

ğŸ“Š Scalability
- As more SSDs were added, BaM scaled linearly in throughput for analytics queries.
- No architectural bottlenecks prevented it from utilizing parallel I/O paths.

ğŸ§  TL;DR for Storage Developers
- Even against DRAM-pinned RAPIDS, BaMâ€™s on-demand, row-granular access won decisively.
- I/O amplification is the silent killer in traditional columnar systems â€” BaM avoids it entirely.
- With proper tuning and SSD parallelism, you donâ€™t need DRAM-scale latency to beat DRAM-based pipelines.

## ğŸš§ Where BaM Didnâ€™t Shine (Yet)
While BaM outperformed DRAM-backed and host-managed systems across most benchmarks, it wasnâ€™t perfect:

**ğŸ§® Vector Addition**
- Workload: Primarily write-heavy
- Result: BaM was 1.5Ã— slower than a simpler GPU baseline
- Reason: The prototype doesn't yet fully overlap read misses with asynchronous writeback â€” a known limitation the authors flag as future work.

**ğŸ› ï¸ GPU Resource Usage**
- Observation: Across all studied workloads (which were I/O-bound), GPU constraints like register pressure or thread limits werenâ€™t the bottleneck.
= Main bottleneck: How fast the system could pull in data from SSDs â€” not how much compute power the GPU had.

## ğŸ§  Final Thoughts: A Paradigm Shift for Storage Software?
The core takeaway from the BAM paper is bold and clear:
Let the GPU drive its own I/O.
What once felt like a far-off architectural dream is now a working prototype â€” built with off-the-shelf GPUs and SSDs â€” and it works shockingly well.

**ğŸš€ Why It Matters**
- Performance Gains: From 45.8M IOPS in microbenchmarks to 1.49Ã— speedup over DRAM systems in graph analytics â€” BAM proves that GPU-initiated storage access can match or beat traditional architectures.
- Cost Advantage: Up to 22Ã— cheaper per GB than DRAM-heavy alternatives â€” without sacrificing performance.
- Architectural Simplicity: No more CPU in the middle micro-managing I/O queues. BAM moves the logic â€” submission queues, completion queues, cache management â€” onto the GPU, close to where the data is used.

**ğŸ”„ Implications for Storage Developers**
- If GPU-initiated I/O becomes mainstream:
- Parts of the OS storage stack may migrate into user-space GPU libraries.
- The boundary between applications, OS, and hardware will blur.
- Tools, abstractions, and debugging paradigms for I/O orchestration will need to evolve to support massively parallel, fine-grained GPU-driven access.

In short, BAM doesn't just optimize a bottleneck â€” it reimagines the memory/storage hierarchy from the GPU's perspective.
This paper is not just a research curiosity â€” it's a blueprint for rethinking I/O architecture for modern, data-intensive GPU workloads.
